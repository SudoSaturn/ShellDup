// Generated by generate.go do not edit
// vim: ft=goasm
//go:build arm64

#include "go_asm.h"
#include "textflag.h"

// func index_byte2_asm_128(data []byte, b1 uint8, b2 uint8) (ans int)
TEXT ·index_byte2_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-40
	// Set all bytes of V0 to the first byte in b1 
	MOVBU b1+24(FP), R0 // load the function parameter b1 into R0 
	VMOV R0, V0.B16
	// Set all bytes of V1 to the first byte in b2 
	MOVBU b2+25(FP), R0 // load the function parameter b2 into R0 
	VMOV R0, V1.B16
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V3.B16] // load memory from the address in R0 to V3 
	VCMEQ V3.B16, V0.B16, V2.B16 // V2 = 0xff on every byte where V3[n] == V0[n] and zero elsewhere 
	VCMEQ V3.B16, V1.B16, V3.B16 // V3 = 0xff on every byte where V3[n] == V1[n] and zero elsewhere 
	VORR V2.B16, V3.B16, V2.B16 // V2 = V2 | V3 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V2 V2 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V2 
	WORD $0xf0c8442
	FMOVD F2, R3 // Extract the lower 64 bits from V2 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V3.B16] // load memory from the address in R0 to V3 
	VCMEQ V3.B16, V0.B16, V2.B16 // V2 = 0xff on every byte where V3[n] == V0[n] and zero elsewhere 
	VCMEQ V3.B16, V1.B16, V3.B16 // V3 = 0xff on every byte where V3[n] == V1[n] and zero elsewhere 
	VORR V2.B16, V3.B16, V2.B16 // V2 = V2 | V3 (bitwise) 
	VDUP V2.D[1], V4 // duplicate the upper 64 bits of V2 into the lower and upper 64 bits of V4 
	VORR V2.B16, V4.B16, V4.B16 // V4 = V2 | V4 (bitwise) 
	FMOVD F4, R4 // R4 = lower 64bits of V4 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V2 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V2 V2 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V2 
	WORD $0xf0c8442
	FMOVD F2, R3 // Extract the lower 64 bits from V2 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+32(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+32(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


// func index_byte2_string_asm_128(data string, b1 uint8, b2 uint8) (ans int)
TEXT ·index_byte2_string_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	// Set all bytes of V0 to the first byte in b1 
	MOVBU b1+16(FP), R0 // load the function parameter b1 into R0 
	VMOV R0, V0.B16
	// Set all bytes of V1 to the first byte in b2 
	MOVBU b2+17(FP), R0 // load the function parameter b2 into R0 
	VMOV R0, V1.B16
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V3.B16] // load memory from the address in R0 to V3 
	VCMEQ V3.B16, V0.B16, V2.B16 // V2 = 0xff on every byte where V3[n] == V0[n] and zero elsewhere 
	VCMEQ V3.B16, V1.B16, V3.B16 // V3 = 0xff on every byte where V3[n] == V1[n] and zero elsewhere 
	VORR V2.B16, V3.B16, V2.B16 // V2 = V2 | V3 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V2 V2 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V2 
	WORD $0xf0c8442
	FMOVD F2, R3 // Extract the lower 64 bits from V2 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V3.B16] // load memory from the address in R0 to V3 
	VCMEQ V3.B16, V0.B16, V2.B16 // V2 = 0xff on every byte where V3[n] == V0[n] and zero elsewhere 
	VCMEQ V3.B16, V1.B16, V3.B16 // V3 = 0xff on every byte where V3[n] == V1[n] and zero elsewhere 
	VORR V2.B16, V3.B16, V2.B16 // V2 = V2 | V3 (bitwise) 
	VDUP V2.D[1], V4 // duplicate the upper 64 bits of V2 into the lower and upper 64 bits of V4 
	VORR V2.B16, V4.B16, V4.B16 // V4 = V2 | V4 (bitwise) 
	FMOVD F4, R4 // R4 = lower 64bits of V4 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V2 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V2 V2 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V2 
	WORD $0xf0c8442
	FMOVD F2, R3 // Extract the lower 64 bits from V2 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+24(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


// func index_c0_asm_128(data []byte) (ans int)
TEXT ·index_c0_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	VCMEQ V0.B16, V0.B16, V0.B16 // V0 = 0xff on every byte where V0[n] == V0[n] and zero elsewhere 
	MOVD $0x20, R0 // R0 =  32 
	// Set all bytes of V1 to the lowest byte in R0 
	VMOV R0, V1.B16
	// 
	MOVD $0x7f, R0 // R0 =  127 
	// Set all bytes of V2 to the lowest byte in R0 
	VMOV R0, V2.B16
	// 
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V4.B16] // load memory from the address in R0 to V4 
	VCMEQ V4.B16, V2.B16, V3.B16 // V3 = 0xff on every byte where V4[n] == V2[n] and zero elsewhere 
	WORD $0x4e243425 // V5 = 0xff on every byte where V1[n] > V4[n] and zero elsewhere 
	WORD $0x4e203484 // V4 = 0xff on every byte where V4[n] > V0[n] and zero elsewhere 
	VAND V5.B16, V4.B16, V4.B16 // V4 = V5 & V4 (bitwise) 
	VORR V3.B16, V4.B16, V3.B16 // V3 = V3 | V4 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V3 V3 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V3 
	WORD $0xf0c8463
	FMOVD F3, R3 // Extract the lower 64 bits from V3 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V4.B16] // load memory from the address in R0 to V4 
	VCMEQ V4.B16, V2.B16, V3.B16 // V3 = 0xff on every byte where V4[n] == V2[n] and zero elsewhere 
	WORD $0x4e243425 // V5 = 0xff on every byte where V1[n] > V4[n] and zero elsewhere 
	WORD $0x4e203484 // V4 = 0xff on every byte where V4[n] > V0[n] and zero elsewhere 
	VAND V5.B16, V4.B16, V4.B16 // V4 = V5 & V4 (bitwise) 
	VORR V3.B16, V4.B16, V3.B16 // V3 = V3 | V4 (bitwise) 
	VDUP V3.D[1], V5 // duplicate the upper 64 bits of V3 into the lower and upper 64 bits of V5 
	VORR V3.B16, V5.B16, V5.B16 // V5 = V3 | V5 (bitwise) 
	FMOVD F5, R4 // R4 = lower 64bits of V5 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V3 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V3 V3 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V3 
	WORD $0xf0c8463
	FMOVD F3, R3 // Extract the lower 64 bits from V3 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+24(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


// func index_c0_string_asm_128(data string) (ans int)
TEXT ·index_c0_string_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-24
	VCMEQ V0.B16, V0.B16, V0.B16 // V0 = 0xff on every byte where V0[n] == V0[n] and zero elsewhere 
	MOVD $0x20, R0 // R0 =  32 
	// Set all bytes of V1 to the lowest byte in R0 
	VMOV R0, V1.B16
	// 
	MOVD $0x7f, R0 // R0 =  127 
	// Set all bytes of V2 to the lowest byte in R0 
	VMOV R0, V2.B16
	// 
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V4.B16] // load memory from the address in R0 to V4 
	VCMEQ V4.B16, V2.B16, V3.B16 // V3 = 0xff on every byte where V4[n] == V2[n] and zero elsewhere 
	WORD $0x4e243425 // V5 = 0xff on every byte where V1[n] > V4[n] and zero elsewhere 
	WORD $0x4e203484 // V4 = 0xff on every byte where V4[n] > V0[n] and zero elsewhere 
	VAND V5.B16, V4.B16, V4.B16 // V4 = V5 & V4 (bitwise) 
	VORR V3.B16, V4.B16, V3.B16 // V3 = V3 | V4 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V3 V3 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V3 
	WORD $0xf0c8463
	FMOVD F3, R3 // Extract the lower 64 bits from V3 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V4.B16] // load memory from the address in R0 to V4 
	VCMEQ V4.B16, V2.B16, V3.B16 // V3 = 0xff on every byte where V4[n] == V2[n] and zero elsewhere 
	WORD $0x4e243425 // V5 = 0xff on every byte where V1[n] > V4[n] and zero elsewhere 
	WORD $0x4e203484 // V4 = 0xff on every byte where V4[n] > V0[n] and zero elsewhere 
	VAND V5.B16, V4.B16, V4.B16 // V4 = V5 & V4 (bitwise) 
	VORR V3.B16, V4.B16, V3.B16 // V3 = V3 | V4 (bitwise) 
	VDUP V3.D[1], V5 // duplicate the upper 64 bits of V3 into the lower and upper 64 bits of V5 
	VORR V3.B16, V5.B16, V5.B16 // V5 = V3 | V5 (bitwise) 
	FMOVD F5, R4 // R4 = lower 64bits of V5 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V3 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V3 V3 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V3 
	WORD $0xf0c8463
	FMOVD F3, R3 // Extract the lower 64 bits from V3 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+16(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+16(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


// func index_byte_asm_128(data []byte, b uint8) (ans int)
TEXT ·index_byte_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-40
	// Set all bytes of V0 to the first byte in b 
	MOVBU b+24(FP), R0 // load the function parameter b into R0 
	VMOV R0, V0.B16
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V2.B16] // load memory from the address in R0 to V2 
	VCMEQ V2.B16, V0.B16, V1.B16 // V1 = 0xff on every byte where V2[n] == V0[n] and zero elsewhere 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V1 V1 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V1 
	WORD $0xf0c8421
	FMOVD F1, R3 // Extract the lower 64 bits from V1 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V2.B16] // load memory from the address in R0 to V2 
	VCMEQ V2.B16, V0.B16, V1.B16 // V1 = 0xff on every byte where V2[n] == V0[n] and zero elsewhere 
	VDUP V1.D[1], V3 // duplicate the upper 64 bits of V1 into the lower and upper 64 bits of V3 
	VORR V1.B16, V3.B16, V3.B16 // V3 = V1 | V3 (bitwise) 
	FMOVD F3, R4 // R4 = lower 64bits of V3 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V1 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V1 V1 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V1 
	WORD $0xf0c8421
	FMOVD F1, R3 // Extract the lower 64 bits from V1 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+32(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+32(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


// func index_byte_string_asm_128(data string, b uint8) (ans int)
TEXT ·index_byte_string_asm_128(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	// Set all bytes of V0 to the first byte in b 
	MOVBU b+16(FP), R0 // load the function parameter b into R0 
	VMOV R0, V0.B16
	MOVD data+0(FP), R1 // load the function parameter data into R1 
	MOVD data_len+8(FP), R2 // load the length of the function parameter data into R2 
	CBZ R2, fail // jump to: fail if R2 is zero 
	ADD R1, R2 // R2 += R1 
	MOVD R1, R0 // R0 = R1 
	MOVD R1, R4 // R4 = R1 
	AND $0xf, R4 // R4 &= 15 
	SUB R4, R0 // R0 -= R4 
	// R0 is now aligned to a 16 byte boundary so loading from it is safe 
	VLD1 (R0), [V2.B16] // load memory from the address in R0 to V2 
	VCMEQ V2.B16, V0.B16, V1.B16 // V1 = 0xff on every byte where V2[n] == V0[n] and zero elsewhere 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V1 V1 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V1 
	WORD $0xf0c8421
	FMOVD F1, R3 // Extract the lower 64 bits from V1 and put them into R3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	// The mask has 4 bits per byte, so multiply R4 by 4 
	LSL $0x2, R4
	LSR R4, R3
	CBZ R3, loop_start // jump to: loop_start if R3 is zero 
	MOVD R1, R0 // R0 = R1 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADD $0x10, R0 // R0 += 16 
	CMP R0, R2 // compare R2 to R0 
	BLE fail // jump to: fail if R2 <= R0 
	VLD1 (R0), [V2.B16] // load memory from the address in R0 to V2 
	VCMEQ V2.B16, V0.B16, V1.B16 // V1 = 0xff on every byte where V2[n] == V0[n] and zero elsewhere 
	VDUP V1.D[1], V3 // duplicate the upper 64 bits of V1 into the lower and upper 64 bits of V3 
	VORR V1.B16, V3.B16, V3.B16 // V3 = V1 | V3 (bitwise) 
	FMOVD F3, R4 // R4 = lower 64bits of V3 
	CBNZ R4, byte_found_in_vec // jump to: byte_found_in_vec if V1 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in R3 
	// Go assembler doesn't support the shrn instruction, below we have: shrn.8b V1 V1 #4 
	// It is shifting right by four bits in every 16 bit word and truncating to 8 bits storing the result in the lower 64 bits of V1 
	WORD $0xf0c8421
	FMOVD F1, R3 // Extract the lower 64 bits from V1 and put them into R3 
	// Get the result from R3 and return it 
byte_found_in_mask: // jump target 
	RBIT R3, R3 // reverse the bits 
	CLZ R3, R3 // R3 = number of leading zeros in R3 
	UBFX $2, R3, $30, R3 // R3 >>= 2 (divide by 4) 
	ADD R0, R3 // R3 += R0 
	CMP R3, R2 // compare R2 to R3 
	BLE fail // jump to: fail if R2 <= R3 
	SUB R1, R3 // R3 -= R1 
	MOVD R3, ans+24(FP) // save the value: R3 to the function return parameter: ans 
	RET // return from function 

fail: // jump target 
	MOVD $-1, R4 // R4 = all ones 
	MOVD R4, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	RET // return from function 


