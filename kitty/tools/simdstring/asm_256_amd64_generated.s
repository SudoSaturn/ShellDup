// Generated by generate.go do not edit
// vim: ft=goasm
//go:build amd64

#include "go_asm.h"
#include "textflag.h"

// func index_byte2_asm_256(data []byte, b1 uint8, b2 uint8) (ans int)
TEXT ·index_byte2_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-40
	// Set all bytes of Y0 to the first byte in b1 
	VPBROADCASTB b1+24(FP), Y0
	// 
	// Set all bytes of Y1 to the first byte in b2 
	VPBROADCASTB b2+25(FP), Y1
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y3 // load memory from the address in AX to Y3 
	VPCMPEQB Y0, Y3, Y2 // Y2 = 0xff on every byte where Y3[n] == Y0[n] and zero elsewhere 
	VPCMPEQB Y1, Y3, Y3 // Y3 = 0xff on every byte where Y3[n] == Y1[n] and zero elsewhere 
	VPOR Y2, Y3, Y2 // Y2 = Y2 | Y3 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y2, SI // SI = mask of the highest bit in every byte in Y2 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y3 // load memory from the address in AX to Y3 
	VPCMPEQB Y0, Y3, Y2 // Y2 = 0xff on every byte where Y3[n] == Y0[n] and zero elsewhere 
	VPCMPEQB Y1, Y3, Y3 // Y3 = 0xff on every byte where Y3[n] == Y1[n] and zero elsewhere 
	VPOR Y2, Y3, Y2 // Y2 = Y2 | Y3 (bitwise) 
	VPTEST Y2, Y2 // test if Y2 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y2 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y2, SI // SI = mask of the highest bit in every byte in Y2 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+32(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+32(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


// func index_byte2_string_asm_256(data string, b1 uint8, b2 uint8) (ans int)
TEXT ·index_byte2_string_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	// Set all bytes of Y0 to the first byte in b1 
	VPBROADCASTB b1+16(FP), Y0
	// 
	// Set all bytes of Y1 to the first byte in b2 
	VPBROADCASTB b2+17(FP), Y1
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y3 // load memory from the address in AX to Y3 
	VPCMPEQB Y0, Y3, Y2 // Y2 = 0xff on every byte where Y3[n] == Y0[n] and zero elsewhere 
	VPCMPEQB Y1, Y3, Y3 // Y3 = 0xff on every byte where Y3[n] == Y1[n] and zero elsewhere 
	VPOR Y2, Y3, Y2 // Y2 = Y2 | Y3 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y2, SI // SI = mask of the highest bit in every byte in Y2 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y3 // load memory from the address in AX to Y3 
	VPCMPEQB Y0, Y3, Y2 // Y2 = 0xff on every byte where Y3[n] == Y0[n] and zero elsewhere 
	VPCMPEQB Y1, Y3, Y3 // Y3 = 0xff on every byte where Y3[n] == Y1[n] and zero elsewhere 
	VPOR Y2, Y3, Y2 // Y2 = Y2 | Y3 (bitwise) 
	VPTEST Y2, Y2 // test if Y2 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y2 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y2, SI // SI = mask of the highest bit in every byte in Y2 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+24(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


// func index_c0_asm_256(data []byte) (ans int)
TEXT ·index_c0_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	VPCMPEQB Y0, Y0, Y0 // Y0 = 0xff on every byte where Y0[n] == Y0[n] and zero elsewhere 
	MOVL $0x20, AX // AX =  32 
	// Set all bytes of Y1 to the lowest byte in AX 
	VMOVD AX, X3
	VPBROADCASTB X3, Y1
	// 
	MOVL $0x7f, AX // AX =  127 
	// Set all bytes of Y2 to the lowest byte in AX 
	VMOVD AX, X3
	VPBROADCASTB X3, Y2
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y4 // load memory from the address in AX to Y4 
	VPCMPEQB Y2, Y4, Y3 // Y3 = 0xff on every byte where Y4[n] == Y2[n] and zero elsewhere 
	VPCMPGTB Y4, Y1, Y5 // Y5 = 0xff on every byte where Y1[n] > Y4[n] and zero elsewhere 
	VPCMPGTB Y0, Y4, Y4 // Y4 = 0xff on every byte where Y4[n] > Y0[n] and zero elsewhere 
	VPAND Y5, Y4, Y4 // Y4 = Y5 & Y4 (bitwise) 
	VPOR Y3, Y4, Y3 // Y3 = Y3 | Y4 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y3, SI // SI = mask of the highest bit in every byte in Y3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y4 // load memory from the address in AX to Y4 
	VPCMPEQB Y2, Y4, Y3 // Y3 = 0xff on every byte where Y4[n] == Y2[n] and zero elsewhere 
	VPCMPGTB Y4, Y1, Y5 // Y5 = 0xff on every byte where Y1[n] > Y4[n] and zero elsewhere 
	VPCMPGTB Y0, Y4, Y4 // Y4 = 0xff on every byte where Y4[n] > Y0[n] and zero elsewhere 
	VPAND Y5, Y4, Y4 // Y4 = Y5 & Y4 (bitwise) 
	VPOR Y3, Y4, Y3 // Y3 = Y3 | Y4 (bitwise) 
	VPTEST Y3, Y3 // test if Y3 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y3 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y3, SI // SI = mask of the highest bit in every byte in Y3 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+24(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


// func index_c0_string_asm_256(data string) (ans int)
TEXT ·index_c0_string_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-24
	VPCMPEQB Y0, Y0, Y0 // Y0 = 0xff on every byte where Y0[n] == Y0[n] and zero elsewhere 
	MOVL $0x20, AX // AX =  32 
	// Set all bytes of Y1 to the lowest byte in AX 
	VMOVD AX, X3
	VPBROADCASTB X3, Y1
	// 
	MOVL $0x7f, AX // AX =  127 
	// Set all bytes of Y2 to the lowest byte in AX 
	VMOVD AX, X3
	VPBROADCASTB X3, Y2
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y4 // load memory from the address in AX to Y4 
	VPCMPEQB Y2, Y4, Y3 // Y3 = 0xff on every byte where Y4[n] == Y2[n] and zero elsewhere 
	VPCMPGTB Y4, Y1, Y5 // Y5 = 0xff on every byte where Y1[n] > Y4[n] and zero elsewhere 
	VPCMPGTB Y0, Y4, Y4 // Y4 = 0xff on every byte where Y4[n] > Y0[n] and zero elsewhere 
	VPAND Y5, Y4, Y4 // Y4 = Y5 & Y4 (bitwise) 
	VPOR Y3, Y4, Y3 // Y3 = Y3 | Y4 (bitwise) 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y3, SI // SI = mask of the highest bit in every byte in Y3 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y4 // load memory from the address in AX to Y4 
	VPCMPEQB Y2, Y4, Y3 // Y3 = 0xff on every byte where Y4[n] == Y2[n] and zero elsewhere 
	VPCMPGTB Y4, Y1, Y5 // Y5 = 0xff on every byte where Y1[n] > Y4[n] and zero elsewhere 
	VPCMPGTB Y0, Y4, Y4 // Y4 = 0xff on every byte where Y4[n] > Y0[n] and zero elsewhere 
	VPAND Y5, Y4, Y4 // Y4 = Y5 & Y4 (bitwise) 
	VPOR Y3, Y4, Y3 // Y3 = Y3 | Y4 (bitwise) 
	VPTEST Y3, Y3 // test if Y3 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y3 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y3, SI // SI = mask of the highest bit in every byte in Y3 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+16(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+16(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


// func index_byte_asm_256(data []byte, b uint8) (ans int)
TEXT ·index_byte_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-40
	// Set all bytes of Y0 to the first byte in b 
	VPBROADCASTB b+24(FP), Y0
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y2 // load memory from the address in AX to Y2 
	VPCMPEQB Y0, Y2, Y1 // Y1 = 0xff on every byte where Y2[n] == Y0[n] and zero elsewhere 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y1, SI // SI = mask of the highest bit in every byte in Y1 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y2 // load memory from the address in AX to Y2 
	VPCMPEQB Y0, Y2, Y1 // Y1 = 0xff on every byte where Y2[n] == Y0[n] and zero elsewhere 
	VPTEST Y1, Y1 // test if Y1 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y1 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y1, SI // SI = mask of the highest bit in every byte in Y1 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+32(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+32(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


// func index_byte_string_asm_256(data string, b uint8) (ans int)
TEXT ·index_byte_string_asm_256(SB), NOSPLIT|TOPFRAME|NOFRAME, $0-32
	// Set all bytes of Y0 to the first byte in b 
	VPBROADCASTB b+16(FP), Y0
	// 
	MOVQ data+0(FP), BX // load the function parameter data into BX 
	MOVQ data_len+8(FP), DX // load the length of the function parameter data into DX 
	TESTQ DX, DX // test if DX is zero 
	JZ fail // jump to: fail if DX is zero 
	ADDQ BX, DX // DX += BX 
	MOVQ BX, AX // AX = BX 
	MOVQ BX, CX // CX = BX 
	ANDQ $0x1f, CX // CX &= 31 
	SUBQ CX, AX // AX -= CX 
	// AX is now aligned to a 32 byte boundary so loading from it is safe 
	VMOVDQA (AX), Y2 // load memory from the address in AX to Y2 
	VPCMPEQB Y0, Y2, Y1 // Y1 = 0xff on every byte where Y2[n] == Y0[n] and zero elsewhere 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y1, SI // SI = mask of the highest bit in every byte in Y1 
	// We need to shift out the possible extra bytes at the start of the string caused by the unaligned read 
	SHRQ CX, SI
	TESTQ SI, SI // test if SI is zero 
	JZ loop_start // jump to: loop_start if SI is zero 
	MOVQ BX, AX // AX = BX 
	JMP byte_found_in_mask // jump to: byte_found_in_mask 
	// Now loop over aligned blocks 
loop_start: // jump target 
	ADDQ $0x20, AX // AX += 32 
	CMPQ DX, AX // compare DX to AX 
	JLE fail // jump to: fail if DX <= AX 
	VMOVDQA (AX), Y2 // load memory from the address in AX to Y2 
	VPCMPEQB Y0, Y2, Y1 // Y1 = 0xff on every byte where Y2[n] == Y0[n] and zero elsewhere 
	VPTEST Y1, Y1 // test if Y1 is zero 
	JNZ byte_found_in_vec // jump to: byte_found_in_vec if Y1 is non-zero 
	JMP loop_start // jump to: loop_start 
byte_found_in_vec: // jump target 
	// Count the number of bytes to the first 0xff byte and put the result in SI 
	VPMOVMSKB Y1, SI // SI = mask of the highest bit in every byte in Y1 
	// Get the result from SI and return it 
byte_found_in_mask: // jump target 
	BSFL SI, SI // SI = number of trailing zeros in SI 
	ADDQ AX, SI // SI += AX 
	CMPQ DX, SI // compare DX to SI 
	JLE fail // jump to: fail if DX <= SI 
	SUBQ BX, SI // SI -= BX 
	MOVQ SI, ans+24(FP) // save the value: SI to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 

fail: // jump target 
	MOVQ $-1, ans+24(FP) // save the value: -1 to the function return parameter: ans 
	VZEROUPPER // zero upper bits of AVX registers to avoid dependencies when switching between SSE and AVX code 
	RET // return from function 


